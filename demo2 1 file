import pandas as pd
import numpy as np
import mne
from scipy.signal import find_peaks, welch
from keras.models import load_model
import ast

# Function to apply a bandpass filter to EEG data
def apply_bandpass_filter(data, l_freq=0.1, h_freq=5.0):
    sfreq = 215.0  # Replace with the correct sampling frequency of your data
    return mne.filter.filter_data(data, sfreq, l_freq, h_freq, method='iir', verbose=False)

# Function to extract features from EEG data
def extract_features_per_channel(data_array):
    features_per_channel = {f"Channel_{i+1}": [] for i in range(4)}
    
    for session_idx in range(len(data_array) // 4):  # 4 channels per session
        for channel_idx in range(4):
            session = data_array[session_idx * 4 + channel_idx]
            
            # Time domain features
            positive_peaks, _ = find_peaks(session)
            negative_peaks, _ = find_peaks(-session)
            peak_to_peak_amplitude = max(session[positive_peaks]) - min(session[negative_peaks]) if positive_peaks.size and negative_peaks.size else 0
            mean_val = np.mean(session)
            variance_val = np.var(session)
            
            # Frequency domain features
            f, Pxx = welch(session, fs=250.0, nperseg=256)
            dominant_frequency = f[np.argmax(Pxx)]
            half_power = np.max(Pxx) / 2
            indices = np.where(Pxx > half_power)
            bandwidth = f[indices[0][-1]] - f[indices[0][0]] if indices[0].size > 0 else 0
            
            # Append features
            features_per_channel[f"Channel_{channel_idx+1}"].append([peak_to_peak_amplitude, mean_val, variance_val, dominant_frequency, bandwidth])

    for key in features_per_channel:
        features_per_channel[key] = np.array(features_per_channel[key])

    return features_per_channel

# Load the trained model
model_path = 'C:/dev/digitalbiomarkers/blink_model.keras'
model = load_model(model_path)

# Load new combined blink data (unlabeled)
new_data_path = 'C:\dev\digitalbiomarkers\EEG-data\ShortBlink.csv'
new_data = pd.read_csv(new_data_path)
new_data['data'] = new_data['data'].apply(ast.literal_eval)

# Process the new data and extract features
new_values = [value for sublist in new_data['data'].tolist() for value in sublist]
new_sessions = [new_values[i:i + 510] for i in range(0, len(new_values), 510)]  # Replace 510 with the correct number of samples per session
new_array = np.array(new_sessions)
new_array_filtered = apply_bandpass_filter(new_array)
new_features = extract_features_per_channel(new_array_filtered)

# Combine features for prediction
all_new_features = np.hstack([new_features[channel] for channel in ["Channel_1", "Channel_2", "Channel_3", "Channel_4"]])

# Predict using the trained model
predictions = model.predict(all_new_features)
predicted_classes = np.argmax(predictions, axis=1)

# Count and output the total number of predicted short and long blinks
short_blink_count = np.sum(predicted_classes == 0)
long_blink_count = np.sum(predicted_classes == 1)

print(f"Short Blink Count: {short_blink_count}")
print(f"Long Blink Count: {long_blink_count}")


